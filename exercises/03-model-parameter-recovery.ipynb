{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a926cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/DevComPsy/cpm.git\n",
    "import cpm\n",
    "from packaging import version\n",
    "\n",
    "## cpm checks\n",
    "print(cpm.__version__)\n",
    "if version.parse(cpm.__version__) < version.parse(\"0.22\"):\n",
    "    raise ImportError(\"cpm version must be >= 0.22. Please install the latest version using: pip install --upgrade cpm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7928c735",
   "metadata": {},
   "source": [
    "# EXERCISE 3\n",
    "\n",
    "Evaluating how well the model recovers the original parameters. Briefly, you will sample a random set of model parameters, generate new data, fit the model to the data, and then compare the estimated parameters to the original ones. This will require some modification to the code you wrote in Exercise 1. Specifically, instead of using the participant's choice to select what value to update, you will take the model's probability of choosing each option and use that to sample a choice. The technique we will use below will also involve two new additions to our model: the _loss function_ and the _Bernoulli distribution function_ to generate data. So, what will you do here?\n",
    "\n",
    "2. Estimate the parameter recovery of the model\n",
    "    1. Generate data from the model\n",
    "    2. Fit the model to the data\n",
    "    3. Compare the estimated parameters to the true parameters\n",
    "\n",
    "## Generating data with the model\n",
    "\n",
    "To generate data with the model, we will use the same _Bernoulli distribution_, with its mean given by the model's probability of choosing each option. The SoftMax function we defined previously will return a probability for each action, and we will sample from this distribution to generate a choice. This is done using the `cpm.models.decision.SoftMax.choice()`. function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583d2015",
   "metadata": {},
   "source": [
    "## EXERCISE 3.1: modify your model, generate data with random parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2183ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import cpm\n",
    "import cpm.datasets as datasets\n",
    "from cpm.generators import Parameters, Value\n",
    "import ipyparallel as ipp  # for parallel computing with ipython (specific for Jupyter Notebook)\n",
    "\n",
    "data = datasets.load_bandit_data()\n",
    "data.head()\n",
    "\n",
    "parameters = Parameters(\n",
    "    # free parameters are indicated by specifying priors\n",
    "    alpha=Value(\n",
    "        value=0.5,\n",
    "        lower=1e-10,\n",
    "        upper=1,\n",
    "        prior=\"truncated_normal\",\n",
    "        args={\"mean\": 0.5, \"sd\": 0.25},\n",
    "    ),\n",
    "    temperature=Value(\n",
    "        value=1,\n",
    "        lower=0,\n",
    "        upper=10,\n",
    "        prior=\"truncated_normal\",\n",
    "        args={\"mean\": 5, \"sd\": 2.5},\n",
    "    ),\n",
    "    # everything without a prior is part of the initial state of the\n",
    "    # model or constructs fixed throughout the simulation\n",
    "    # (e.g. exemplars in general-context models of categorizations)\n",
    "    # initial q-values starting starting from non-zero value\n",
    "    # these are equal to all 4 stimuli (1 / 4)\n",
    "    values = numpy.array([0.25, 0.25, 0.25, 0.25])\n",
    "    )\n",
    "\n",
    "@ipp.require(\"numpy\")\n",
    "def model(parameters, trial):\n",
    "    # pull out the parameters\n",
    "    alpha = parameters.alpha\n",
    "    temperature = parameters.temperature\n",
    "    values = numpy.array(parameters.values)\n",
    "    \n",
    "    # pull out the trial information\n",
    "    stimulus = numpy.array([trial.arm_left, trial.arm_right]).astype(int)\n",
    "    feedback = numpy.array([trial.reward_left, trial.reward_right])\n",
    "\n",
    "    # Equation 1. - get the value of each available action\n",
    "    # Note that because python counts from 0, we need to shift\n",
    "    # the stimulus identifiers by -1\n",
    "    expected_rewards = values[stimulus - 1]\n",
    "    # convert columns to rows\n",
    "    expected_rewards = expected_rewards.reshape(2, 1)\n",
    "    # calculate a policy based on the activations\n",
    "    # Equation 2.\n",
    "    ## you will need expected rewards and temperature\n",
    "    ## look at the function documentations provided above\n",
    "    _____________________________\n",
    "    # after that, you need to compute the policy with the .compute method\n",
    "    _____________________________\n",
    "    # if the policy is NaN for an action, then we need to set it to 1\n",
    "    # this corrects some numerical issues with python and infinities\n",
    "    if numpy.isnan(choice_rule.policies).any():\n",
    "        choice_rule.policies[numpy.isnan(choice_rule.policies)] = 1\n",
    "    ## find the relevant method of SoftMax in its documentation\n",
    "    model_choices = _________________________  # get the model's choice\n",
    "    # get the received reward for the choice\n",
    "    reward = feedback[model_choices]\n",
    "    teacher = numpy.array([reward])\n",
    "    # we now create a vector that tells our learning rule what...\n",
    "    # ... stimulus to update according to the participant's choice\n",
    "    what_to_update = numpy.zeros(4)\n",
    "    chosen_stimulus = stimulus[model_choices] - 1\n",
    "    what_to_update[chosen_stimulus] = 1\n",
    "\n",
    "    # Equation 4.\n",
    "    update = cpm.models.learning.SeparableRule(\n",
    "                    weights=values,\n",
    "                    feedback=teacher,\n",
    "                    input=what_to_update,\n",
    "                    alpha=alpha\n",
    "                    )\n",
    "    update.compute()\n",
    "    # Equation 5.\n",
    "    values += update.weights.flatten()\n",
    "    # compile output\n",
    "    output = {\n",
    "        \"trial\"    : trial.trial.astype(int), # trial numbers\n",
    "        \"activation\" : expected_rewards.flatten(), # expected reward of arms\n",
    "        \"policy\"   : choice_rule.policies,       # policies\n",
    "        \"reward\"   : reward,                  # received reward\n",
    "        \"error\"    : update.weights,          # prediction error\n",
    "        \"values\"   : values,                  # updated values\n",
    "        \"response\" : model_choices,          # model's choice\n",
    "        # dependent variable\n",
    "        \"dependent\"  : numpy.array([choice_rule.policies[1]]),\n",
    "    }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08399d15",
   "metadata": {},
   "source": [
    "# EXERCISE 3.2: generate data with random parameters\n",
    "\n",
    "Her, you need to generate a random set of parameters, equal in number to the number of participants in your data set. Then, you will use the `cpm.generators.Simulator` class to generate data for the whole sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb19520",
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model = cpm.generators.Wrapper(\n",
    "    model=model,\n",
    "    parameters=parameters,\n",
    "    data=data[data.ppt == 1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e8e93d",
   "metadata": {},
   "source": [
    "In the code below, you will need to fill in the missing parts by sampling from the parameters space. You can use the `cpm.generators.Parameters` class to sample parameters from the priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee49754",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_participants = data.ppt.nunique()\n",
    "parameters_for_the_whole_sample = parameters.sample(size=number_of_participants)\n",
    "original_parameters = pd.DataFrame(parameters_for_the_whole_sample) ## save the original parameters\n",
    "original_parameters[\"ppt\"] = data.ppt.unique()\n",
    "\n",
    "# create a simulator that will run the model for each participant\n",
    "simulator = cpm.generators.Simulator(\n",
    "    wrapper=generative_model,\n",
    "    data=data.groupby(\"ppt\"),\n",
    "    parameters=parameters_for_the_whole_sample,\n",
    ")\n",
    "simulator.run()\n",
    "# collect the results\n",
    "results = simulator.export()\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54993839",
   "metadata": {},
   "source": [
    "All that is left from the simulations is to take the model output and create our dependent variable for the fitting. The toolbox requires the data to contain a single column with the `dependent variable`, that we are estimating model performance against. In this case, the dependent variable is the choice made by the model, which is a binary variable (0 or 1) indicating whether the model chose option 1 or option 2. We take that from the `Simulator` output and create a new column in the data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff5e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_data = data.copy() ## copy the original data\n",
    "## use recovery_data to store the new observed variable and responses \n",
    "_________________________ = _____________________ ## copy the observed variable of interest\n",
    "_________________________ = _____________________ ## copy the model's response\n",
    "\n",
    "recovery_data.head() ## print the first few rows of the data to check if it looks right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bcae4d",
   "metadata": {},
   "source": [
    "## EXERCISE 3.2: fit the model to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1668b4",
   "metadata": {},
   "source": [
    "## EXERCISE 3.2.1: revert the model to the original version\n",
    "\n",
    "So, we need to revert the model to the original version, which means removing the `choice` argument from the code we wrote before. This will allow us to fit the model to the data generated in Exercise 2.2. as opposed to generating new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60804b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ipp.require(\"numpy\")\n",
    "def model_fitting(parameters, trial):\n",
    "    # pull out the parameters\n",
    "    alpha = parameters.alpha\n",
    "    temperature = parameters.temperature\n",
    "    values = numpy.array(parameters.values)\n",
    "    \n",
    "    # pull out the trial information\n",
    "    stimulus = numpy.array([trial.arm_left, trial.arm_right]).astype(int)\n",
    "    feedback = numpy.array([trial.reward_left, trial.reward_right])\n",
    "    human_choice = trial.observed.astype(int)\n",
    "\n",
    "    # Equation 1. - get the value of each available action\n",
    "    # Note that because python counts from 0, we need to shift\n",
    "    # the stimulus identifiers by -1\n",
    "    expected_rewards = values[stimulus - 1]\n",
    "    # convert columns to rows\n",
    "    expected_rewards = expected_rewards.reshape(2, 1)\n",
    "    # calculate a policy based on the activations\n",
    "    # Equation 2.\n",
    "    choice_rule = cpm.models.decision.Softmax(\n",
    "        activations=expected_rewards,\n",
    "        temperature=temperature\n",
    "        )\n",
    "    choice_rule.compute() # compute the policy\n",
    "    # if the policy is NaN for an action, then we need to set it to 1\n",
    "    # this corrects some numerical issues with python and infinities\n",
    "    if numpy.isnan(choice_rule.policies).any():\n",
    "        choice_rule.policies[numpy.isnan(choice_rule.policies)] = 1\n",
    "    # get the received reward for the choice\n",
    "    reward = feedback[human_choice]\n",
    "    teacher = numpy.array([reward])\n",
    "    # we now create a vector that tells our learning rule what...\n",
    "    # ... stimulus to update according to the participant's choice\n",
    "    what_to_update = numpy.zeros(4)\n",
    "    chosen_stimulus = stimulus[human_choice] - 1\n",
    "    what_to_update[chosen_stimulus] = 1\n",
    "\n",
    "    # Equation 4.\n",
    "    update = cpm.models.learning.SeparableRule(\n",
    "                    weights=values,\n",
    "                    feedback=teacher,\n",
    "                    input=what_to_update,\n",
    "                    alpha=alpha\n",
    "                    )\n",
    "    update.compute()\n",
    "    # Equation 5.\n",
    "    values += update.weights.flatten()\n",
    "    # compile output\n",
    "    output = {\n",
    "        \"trial\"    : trial.trial.astype(int), # trial numbers\n",
    "        \"activation\" : expected_rewards.flatten(), # expected reward of arms\n",
    "        \"policy\"   : choice_rule.policies,       # policies\n",
    "        \"reward\"   : reward,                  # received reward\n",
    "        \"error\"    : update.weights,          # prediction error\n",
    "        \"values\"   : values,                  # updated values\n",
    "        # dependent variable\n",
    "        \"dependent\"  : numpy.array([choice_rule.policies[1]]),\n",
    "    }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef4b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = cpm.generators.Wrapper(\n",
    "    model=model_fitting,\n",
    "    parameters=parameters,\n",
    "    data=recovery_data[recovery_data.ppt == 1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1778145",
   "metadata": {},
   "source": [
    "Now that we have everything ready, we can fit the model to the data. We will use the `cpm.optimisation.FminBound` class to do that. The fitting process will return a set of estimated parameters for each participant, which we can then compare to the original parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edd6fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cpm.optimisation import minimise, FminBound\n",
    "# Set up the fitting procedure\n",
    "fit = FminBound(\n",
    "    model=base_model,  # Wrapper class with the model we specified from before\n",
    "    data=recovery_data.groupby('ppt'),  # the data as a list of dictionaries\n",
    "    minimisation=minimise.LogLikelihood.bernoulli,\n",
    "    parallel=True,\n",
    "    libraries=[\"numpy\", \"cpm\", \"pandas\"],\n",
    "    prior=False,\n",
    "    ppt_identifier=\"ppt\",\n",
    "    display=False,\n",
    "    number_of_starts=5,\n",
    "    # everything below is optional and passed directly to the scipy implementation of the optimiser\n",
    "    approx_grad=True\n",
    "\n",
    ")\n",
    "\n",
    "fit.optimise()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8fc088",
   "metadata": {},
   "source": [
    "## EXERCISE 3.3: compare the estimated parameters with the original ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5561acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_parameters = fit.export()\n",
    "recovered_parameters.rename(\n",
    "    columns={\n",
    "        \"x_0\": \"alpha\",\n",
    "        \"x_1\": \"temperature\",\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "recovered_parameters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391d91ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "alpha_original = original_parameters[\"alpha\"].values\n",
    "alpha_recovered = recovered_parameters[\"alpha\"].values\n",
    "beta_original = original_parameters[\"temperature\"].values\n",
    "beta_recovered = recovered_parameters[\"temperature\"].values\n",
    "# Plotting the results\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=False)\n",
    "\n",
    "## scatterplots\n",
    "axes[0].scatter(alpha_original, alpha_recovered, alpha=0.5)\n",
    "## add correlation coefficient\n",
    "corr_alpha = numpy.corrcoef(alpha_original, alpha_recovered)[0, 1]\n",
    "axes[0].text(0.5, 0.9, f'Correlation: {corr_alpha:.2f}', transform=axes[0].transAxes, fontsize=12, ha='center', va='center')\n",
    "axes[0].set_xlabel(r'Original $\\alpha$')\n",
    "axes[0].set_ylabel(r'Recovered $\\alpha$')\n",
    "axes[0].set_title('Learning rate recovery\\n')\n",
    "axes[0].plot([0, 1], [0, 1], 'r--')  # Diagonal line for reference\n",
    "axes[1].scatter(beta_original, beta_recovered, alpha=0.5)\n",
    "## add correlation coefficient\n",
    "corr_beta = numpy.corrcoef(beta_original, beta_recovered)[0, 1]\n",
    "axes[1].text(0.5, 0.9, f'Correlation: {corr_beta:.2f}', transform=axes[1].transAxes, fontsize=12, ha='center', va='center')\n",
    "axes[1].set_xlabel(r'Original $\\beta$')\n",
    "axes[1].set_ylabel(r'Recovered $\\beta$')\n",
    "axes[1].set_title('Choice stochasticity (inverse temperature) recovery\\n')\n",
    "axes[1].plot([0, 10], [0, 10], 'r--')  # Diagonal line for reference\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b10696",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "- What do you think about the results of the parameter recovery? Are the parameters identifiable?\n",
    "- Are there any parameters that are not identifiable? If so, why do you think that is the case?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
