{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e7b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cpm\n",
    "from packaging import version\n",
    "\n",
    "## cpm checks\n",
    "print(cpm.__version__)\n",
    "if version.parse(cpm.__version__) < version.parse(\"0.22\"):\n",
    "    raise ImportError(\"cpm version must be >= 0.22. Please install the latest version using: pip install --upgrade cpm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927a1fe8",
   "metadata": {},
   "source": [
    "# EXERCISE 1\n",
    "\n",
    "In the following exercise, you will implement a model based on the mathematical description, and use the `cpm` toolbox to do so. What will you do here?\n",
    "\n",
    "1. Build a model of a simple bandit task using the toolbox based on the mathematical description\n",
    "1. Explore the model's behaviour by varying its parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65144c31",
   "metadata": {},
   "source": [
    "## The model description\n",
    "\n",
    "\n",
    "Let each stimulus have an associated value, which is the expected reward that can be obtained from selecting that stimulus. Let also $Q(a)$ be the estimated value of action $a$. We set the starting value for all $Q(a)$ to be nonzero and equally distributed between all stimuli.\n",
    "\n",
    "In each trial, $t$, there are two stimuli present, so $Q(a)$ could be $Q(\\text{left})$ or $Q(\\text{right})$, where the corresponding Q values are derived from the associated value of the stimuli present on the left or right.\n",
    "More formally, we can say that the expected value of the action $a$ selected at time $t$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "Q_t(a) = \\mathbb{E}[R_t | A_t = a]\n",
    "\\end{equation}\n",
    "\n",
    "where $R_t$ is the reward received at time $t$, and $A_t$ is the action selected at time $t$. In each trial $t$, the Softmax choice rule (Bridle, 1990) will select an action (left or right) based on the following policy:\n",
    "\n",
    "\\begin{equation}\n",
    "P(a_t) = \\frac{e^{Q_{a,t} \\beta}}{\\sum_{i = 1}^{k}{e^{Q_{i,t} \\beta}}}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\beta$ is the inverse temperature parameter, also referred to as choice stochasticity, and $Q_{a,t}$ is the estimated value of the action $a$ at time $t$. $k$ is the number of actions available, and in our case, $k = 2$. The model uses the variant of the delta rule (Rescorla & Wagner, 1972; Rumelhart, Hinton & Williams, 1986) adapted for multi-armed bandit problems where each option has a single dimension (Barto & Sutton, 2018), reducing Rescorla-Wagner's summed error-term to the following equation, similar to single linear operators (Bush and Mosteller, 1955):\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta Q_t(A_t) = \\alpha \\times \\Big[ R_t - Q_t(A_t) \\Big]\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where $\\alpha$ is the learning rate and $R_t$ is the reward received at time $t$, also called a teaching signal and sometimes annotated as $\\lambda$. $A_t$ is the action chosen for the trial $t$. Then we update the Q-values, such as:\n",
    "\n",
    "\\begin{equation}\n",
    "Q_{t+1}(A_t) = Q_t(A_t) + \\Delta Q_t(A_t)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f24c69",
   "metadata": {},
   "source": [
    "## EXERCISE 1.1: build your model\n",
    "\n",
    "Your task here will be to implement the model described above using the `cpm` toolbox. Fortunately, most of the code is already here, so you will only need to fill in the blanks.\n",
    "\n",
    "First, let us look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e7c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cpm.datasets as datasets\n",
    "\n",
    "data = datasets.load_bandit_data()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72265b2d",
   "metadata": {},
   "source": [
    "The model will process each trial in the data, so below we can actually see what the model is going to see when it is run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a48d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa4d81",
   "metadata": {},
   "source": [
    "Now let's start by the model parameters. Specify each model parameter, their respective priors, and the initial values. The model parameters are:\n",
    "- `alpha`: the learning rate\n",
    "- `beta`: the inverse temperature parameter\n",
    "- `Q`: the initial Q-values for each action (not a free parameter, but a model state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b130a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cpm.generators import Parameters, Value\n",
    "import numpy\n",
    "import pandas as pd\n",
    "\n",
    "parameters = Parameters(\n",
    "    # free parameters are indicated by specifying priors\n",
    "    alpha=Value(\n",
    "        _________________\n",
    "    ),\n",
    "    temperature=Value(\n",
    "        _________________\n",
    "    ),\n",
    "    # everything without a prior is part of the initial state of the\n",
    "    values=_______________\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c5b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cpm\n",
    "import ipyparallel as ipp  ## for parallel computing with ipython (specific for Jupyter Notebook)\n",
    "\n",
    "@ipp.require(\"numpy\")\n",
    "def model(parameters, trial):\n",
    "    # pull out the parameters\n",
    "    alpha = __________________________\n",
    "    temperature = __________________________\n",
    "    values = __________________________\n",
    "    \n",
    "    # pull out the trial information\n",
    "    stimulus = numpy.array([trial.arm_left, trial.arm_right]).astype(int)\n",
    "    feedback = numpy.array([trial.reward_left, trial.reward_right])\n",
    "    human_choice = __________________________\n",
    "\n",
    "    # Equation 1. - get the value of each available action\n",
    "    # Note that because python counts from 0, we need to shift\n",
    "    # the stimulus identifiers by -1\n",
    "    ___________________________________\n",
    "    # calculate a policy based on the activations\n",
    "    # Equation 2.\n",
    "    ___________________________________\n",
    "    # if the policy is NaN for an action, then we need to set it to 1\n",
    "    # this corrects some numerical issues with python and infinities\n",
    "    ___________________________________\n",
    "    # get the received reward for the choice\n",
    "    __________________________________\n",
    "    # we now create a vector that tells our learning rule what...\n",
    "    # ... stimulus to update according to the participant's choice\n",
    "    what_to_update = __________________________\n",
    "\n",
    "    # Equation 4.\n",
    "    __________________________\n",
    "    # Equation 5.\n",
    "    values += update.weights.flatten()\n",
    "    # compile output\n",
    "    output = {\n",
    "        __________________________\n",
    "        # dependent variable is a mandatory field - you have to decide what \n",
    "        \"dependent\"  : __________________________\n",
    "    }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9009c4",
   "metadata": {},
   "source": [
    "One important thing to note is that the model requires to output a variable called `dependent_variable`, which is the prediction we wish to compare to observations. Once you filled in the blanks, you can run the model and see how it performs on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4379b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(parameters, data.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4736648d",
   "metadata": {},
   "source": [
    "## EXERCISE 1.2: run your model with different parameters and explore its behaviour\n",
    "\n",
    "Here, we will see how the model behaves with different parameters. You can change the values of `alpha` and `beta` to see how they affect the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bfa1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cpm.generators import Simulator, Wrapper\n",
    "\n",
    "wrapper = Wrapper(______________________________)\n",
    "wrapper.run()\n",
    "wrapper.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32d7f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.reset(parameters=_______________, data=_______________)\n",
    "wrapper.run()\n",
    "wrapper.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74830c2",
   "metadata": {},
   "source": [
    "## EXERCISE 1.3: run your model with different starting Q-values and explore its behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e799ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.reset(parameters=_______________, data=________________)\n",
    "wrapper.run()\n",
    "wrapper.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ddecea",
   "metadata": {},
   "source": [
    "## Exercise 1.4: plot the model output as a function of change in the learning rate\n",
    "\n",
    "Pick a range of learning rates and plot the model output as a function of the change in the learning rate. Try to find learning rates that lead to different behaviours of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c93826",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_not_so_random = numpy.array([________________________])\n",
    "\n",
    "big_results = pd.DataFrame()\n",
    "\n",
    "for i in numpy.arange(len(alpha_not_so_random)):\n",
    "    print(f\"Running simulation for participant {i + 1} with alpha={alpha_not_so_random[i]}\")\n",
    "    wrapper.reset(________________________________________________)\n",
    "    wrapper.run()\n",
    "    output = wrapper.export()\n",
    "    output[\"alpha\"] = alpha_not_so_random[i]\n",
    "    big_results = pd.concat([big_results, output], ignore_index=True)\n",
    "\n",
    "big_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85efd2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "value_cols = ['values_0', 'values_1', 'values_2', 'values_3']\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n",
    "\n",
    "for idx, alpha in enumerate(alpha_not_so_random):\n",
    "    ax = axes[idx]\n",
    "    subset = big_results[big_results['alpha'] == alpha]\n",
    "    for vcol, color in zip(value_cols, colors):\n",
    "        ax.plot(subset['trial_0'], subset[vcol], label=vcol, color=color)\n",
    "    ax.set_ylabel('Q-value')\n",
    "    ax.set_title(f'alpha={alpha}')\n",
    "    ax.legend(['stimulus 1', 'stimulus 2', 'stimulus 3', 'stimulus 4'], loc='upper left')\n",
    "axes[-1].set_xlabel('Trial')\n",
    "fig.suptitle('Evolution of Q-values for Different Learning Rates\\n', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b9289",
   "metadata": {},
   "source": [
    "- Are there any learning rates that lead to a stable behaviour of the model? If so, what are they?\n",
    "- What does it mean that the model has unstable behaviour?\n",
    "- How can you interpret the model's behaviour in terms of the learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f616657f",
   "metadata": {},
   "source": [
    "## Simulating different participants with same and different parameters\n",
    "\n",
    "There are built-in tools in the `cpm` toolbox to allow you to explore model behaviour in a variety of ways. The process of trying to understand how the model explains the data often involves exploring its parameter space, simulating different trial orders, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e3a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = data[data.ppt.isin([1, 3, 9, 4, 10])].copy()\n",
    "numpy.random.seed(42)\n",
    "multiple = parameters.sample(5) ## get 5 random parameter sets for each participant\n",
    "\n",
    "simulate =  cpm.generators.Simulator(\n",
    "    ________________________________,\n",
    ")\n",
    "simulate.run()\n",
    "simulations_multiple_ppt = simulate.export()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ba44ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## you can just run it - it will plot the results for you\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "participants = simulations_multiple_ppt['ppt'].unique()\n",
    "n_participants = len(participants)\n",
    "\n",
    "fig, axes = plt.subplots(n_participants, 1, figsize=(12, 3 * n_participants), sharex=True)\n",
    "\n",
    "value_cols = ['values_0', 'values_1', 'values_2', 'values_3']\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n",
    "\n",
    "for idx, ppt in enumerate(participants):\n",
    "    ax = axes[idx] if n_participants > 1 else axes\n",
    "    subset = simulations_multiple_ppt[simulations_multiple_ppt['ppt'] == ppt]\n",
    "    for vcol, color in zip(value_cols, colors):\n",
    "        ax.plot(subset['trial_0'], subset[vcol], label=vcol, color=color)\n",
    "    ax.set_ylabel('Q-value')\n",
    "    ax.set_title(f'Participant {ppt} with alpha={numpy.round(multiple[idx].get(\"alpha\"), 3)}')\n",
    "\n",
    "fig.legend(['stimulus 1', 'stimulus 2', 'stimulus 3', 'stimulus 4'], loc='upper right')\n",
    "axes[-1].set_xlabel('Trial')\n",
    "fig.suptitle('Evolution of Q-values for Different Participants\\n', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d7dbf",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "* What do you notice here? What do you think about the model's behaviour?\n",
    "* How do the parameters affect the model's predictions?\n",
    "* How do the initial Q-values affect the model's predictions?\n",
    "* Anything that surprises you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6163a2df",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Barto AG, Sutton RS. Reinforcement learning: An introduction. 2nd ed. The MIT Press; 2018.\n",
    "\n",
    "Bridle JS. Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. In: Neurocomputing: Algorithms, architectures and applications. Springer; 1990. p. 227–236.\n",
    "\n",
    "Bush RR, Mosteller F. A mathematical model for simple learning. Psychological review. 1951;58(5):313.\n",
    "\n",
    "Rescorla RA, Wagner AR. A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement. In: Black AH, Prokasy WF, editors. Classical Conditioning II: Current Research and Theory. Appleton-Century-Crofts; 1972. p. 64–99.\n",
    "\n",
    "Rumelhart DE, Hinton GE, Williams RJ. Learning representations by back-propagating errors. nature. 1986;323(6088):533–536.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
