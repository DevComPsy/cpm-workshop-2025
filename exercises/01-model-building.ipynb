{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e7b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cpm-toolbox\n",
    "import cpm\n",
    "from packaging import version\n",
    "\n",
    "## cpm checks\n",
    "print(cpm.__version__)\n",
    "if version.parse(cpm.__version__) < version.parse(\"0.22\"):\n",
    "    raise ImportError(\"cpm version must be >= 0.22. Please install the latest version using: pip install --upgrade cpm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927a1fe8",
   "metadata": {},
   "source": [
    "# EXERCISE 1\n",
    "\n",
    "In the following exercise, you will implement a model based on the mathematical description, and use the `cpm` toolbox to do so. What will you do here?\n",
    "\n",
    "1. Build a model of a simple bandit task using the toolbox based on the mathematical description\n",
    "1. Explore the model's behaviour by varying its parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65144c31",
   "metadata": {},
   "source": [
    "## The model description\n",
    "\n",
    "\n",
    "Let each stimulus have an associated value, which is the expected reward that can be obtained from selecting that stimulus. Let also $Q(a)$ be the estimated value of action $a$. We set the starting value for all $Q(a)$ to be nonzero and equally distributed between all stimuli.\n",
    "\n",
    "In each trial, $t$, there are two stimuli present, so $Q(a)$ could be $Q(\\text{left})$ or $Q(\\text{right})$, where the corresponding Q values are derived from the associated value of the stimuli present on the left or right.\n",
    "More formally, we can say that the expected value of the action $a$ selected at time $t$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "Q_t(a) = \\mathbb{E}[R_t | A_t = a]\n",
    "\\end{equation}\n",
    "\n",
    "where $R_t$ is the reward received at time $t$, and $A_t$ is the action selected at time $t$. In each trial $t$, the Softmax choice rule (Bridle, 1990) conceptually related to Luce's choice axiom (Luce, 1959), will assign probabilities to action (left or right) based on the following policy:\n",
    "\n",
    "\\begin{equation}\n",
    "P(a_t) = \\frac{e^{Q_{a,t} \\beta}}{\\sum_{i = 1}^{k}{e^{Q_{i,t} \\beta}}}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\beta$ is the inverse temperature parameter, also referred to as choice stochasticity, and $Q_{a,t}$ is the estimated value of the action $a$ at time $t$. $k$ is the number of actions available, and in our case, $k = 2$. The model uses the variant of the delta rule (Rescorla & Wagner, 1972; Rumelhart, Hinton & Williams, 1986) adapted for multi-armed bandit problems where each option has a single dimension (Barto & Sutton, 2018), reducing Rescorla-Wagner's summed error-term to the following equation, similar to single linear operators (Bush and Mosteller, 1955):\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta Q_t(A_t) = \\alpha \\times \\Big[ R_t - Q_t(A_t) \\Big]\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where $\\alpha$ is the learning rate and $R_t$ is the reward received at time $t$, also called a teaching signal and sometimes annotated as $\\lambda$. $A_t$ is the action chosen for the trial $t$. Then we update the Q-values, such as:\n",
    "\n",
    "\\begin{equation}\n",
    "Q_{t+1}(A_t) = Q_t(A_t) + \\Delta Q_t(A_t)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f24c69",
   "metadata": {},
   "source": [
    "## Explore your data\n",
    "\n",
    "Your task here will be to implement the model described above using the `cpm` toolbox. Fortunately, most of the code is already here, so you will only need to fill in the blanks.\n",
    "\n",
    "First, let us look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e7c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cpm.datasets as datasets\n",
    "\n",
    "data = datasets.load_bandit_data()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72265b2d",
   "metadata": {},
   "source": [
    "The model will process each trial in the data, so below we can actually see what the model is going to see when it is run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a48d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648f4b25",
   "metadata": {},
   "source": [
    "## EXERCISE 1.1A: Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa4d81",
   "metadata": {},
   "source": [
    "Now let's start by the model parameters. Specify each model parameter, their respective priors, and the initial values. The model parameters are:\n",
    "- `alpha`: the learning rate\n",
    "- `temperature`: the inverse temperature parameter\n",
    "- `Q`: the initial Q-values for each action (not a free parameter, but a model state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b130a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cpm.generators import Parameters, Value\n",
    "import numpy\n",
    "import pandas as pd\n",
    "\n",
    "parameters = Parameters(\n",
    "    # free parameters are indicated by specifying priors\n",
    "    alpha=Value(\n",
    "    ____________________ \n",
    "    ),\n",
    "    temperature=Value(\n",
    "    _____________________\n",
    "    ),\n",
    "    # everything without a prior is part of the initial state of the\n",
    "    # model or constructs fixed throughout the simulation\n",
    "    # (e.g. exemplars in general-context models of categorizations)\n",
    "    # initial q-values starting starting from non-zero value\n",
    "    # these are equal to all 4 stimuli (1 / 4)\n",
    "    Qvalues = _______________________________\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd876e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters.export()\n",
    "parameters.sample(5)\n",
    "parameters.bounds()\n",
    "parameters.update(alpha=0.6, temperature=2.5)\n",
    "parameters.alpha.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34c23bd",
   "metadata": {},
   "source": [
    "## EXERCISE 1.1B: Model Implementation\n",
    "\n",
    "Fill out the missing processes in the model. The model will be implemented as a simple function. You will need to implement the following methods:\n",
    "\n",
    "1. Learning rule. The learning rule will be calculating the change in Q-values based on the reward received and the current Q-value for the action selected. You will be using a simple delta rule, which we already implemented in [`cpm.models.learning.SeparableRule`](https://devcompsy.github.io/cpm/references/models/#cpm.models.learning.SeparableRule).\n",
    "2. Choice rule. The choice rule will be selecting the action based on the Q-values and the inverse temperature parameter. You will be using a softmax choice rule, which we already implemented in [`cpm.models.decision.Softmax`](https://devcompsy.github.io/cpm/references/models/#cpm.models.decision.Softmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c5b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cpm\n",
    "import ipyparallel as ipp  ## for parallel computing with ipython (specific for Jupyter Notebook)\n",
    "\n",
    "@ipp.require(\"numpy\")\n",
    "def model(parameters, trial):\n",
    "    # pull out the parameters\n",
    "    alpha = parameters.alpha\n",
    "    temperature = parameters.temperature\n",
    "    values = numpy.array(parameters.values)\n",
    "    \n",
    "    # pull out the trial information\n",
    "    stimulus = numpy.array([trial.arm_left, trial.arm_right]).astype(int)\n",
    "    feedback = numpy.array([trial.reward_left, trial.reward_right])\n",
    "    human_choice = trial.response.astype(int)\n",
    "\n",
    "    # Equation 1. - get the value of each available action\n",
    "    # Note that because python counts from 0, we need to shift\n",
    "    # the stimulus identifiers by -1\n",
    "    expected_rewards = values[stimulus - 1]\n",
    "    # convert columns to rows\n",
    "    expected_rewards = expected_rewards.reshape(2, 1)\n",
    "    # calculate a policy based on the activations\n",
    "    # Equation 2.\n",
    "    ## you will need expected rewards and temperature\n",
    "    ## look at the function documentations provided above\n",
    "    _____________________________\n",
    "    # after that, you need to compute the policy with the .compute method\n",
    "    ____________________________\n",
    "    # if the policy is NaN for an action, then we need to set it to 1\n",
    "    # this corrects some numerical issues with python and infinities\n",
    "    if numpy.isnan(choice_rule.policies).any():\n",
    "        choice_rule.policies[numpy.isnan(choice_rule.policies)] = 1\n",
    "    # get the received reward for the choice\n",
    "    reward = feedback[human_choice]\n",
    "    reward = numpy.array([reward])\n",
    "    # we now create a vector that tells our learning rule what...\n",
    "    # ... stimulus to update according to the participant's choice\n",
    "    what_to_update = numpy.zeros(4)\n",
    "    chosen_stimulus = stimulus[human_choice] - 1\n",
    "    what_to_update[chosen_stimulus] = 1\n",
    "\n",
    "    # Equation 4.\n",
    "    # update the values based on the received reward\n",
    "    ## you will need the:\n",
    "    # learning rate (alpha)\n",
    "    # values: Q-values\n",
    "    # reward: received reward for the choice\n",
    "    # what_to_update: telling the function what q-value you are updating \n",
    "    ____________________________\n",
    "    # Equation 5.\n",
    "    values += update.weights.flatten()\n",
    "    # compile output\n",
    "    output = {\n",
    "        \"trial\"    : trial.trial.astype(int), # trial numbers\n",
    "        \"activation\" : expected_rewards.flatten(), # expected reward of arms\n",
    "        \"policy\"   : _________________,       # policies\n",
    "        \"reward\"   : reward,                  # received reward\n",
    "        \"error\"    : update.weights,          # prediction error\n",
    "        \"Qvalues\"   : values,                  # updated values\n",
    "        # dependent variable\n",
    "        \"dependent\"  : numpy.array([choice_rule.policies[1]]),\n",
    "    }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9009c4",
   "metadata": {},
   "source": [
    "One important thing to note is that the model requires to output a variable called `dependent_variable`, which is the prediction we wish to compare to observations. Once you filled in the blanks, you can run the model and see how it performs on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4379b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(parameters, data.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4736648d",
   "metadata": {},
   "source": [
    "## EXERCISE 1.2: run your model with different parameters and explore its behaviour\n",
    "\n",
    "Here, we will see how the model behaves with different parameters. You can change the values of `alpha` and `temperature` to see how they affect the model's predictions. First we will input the model, parameters, and data into the model wrapper. The wrapper will take care of running the model and exporting the results. We will use the `cpm.generators.Wrapper` class to do this. If you need more information, read the documentation [here](https://devcompsy.github.io/cpm/references/generators/#cpm.generators.Wrapper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bfa1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cpm.generators import Simulator, Wrapper\n",
    "\n",
    "wrapper = Wrapper(model=model, parameters=parameters, data=data[data.ppt == 1])\n",
    "wrapper.run()\n",
    "wrapper.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32d7f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.reset(parameters={__________________}, data=________________)\n",
    "wrapper.run()\n",
    "wrapper.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b401d",
   "metadata": {},
   "source": [
    "In the following code, you will need to change the initial Q-values for the stimuli. Do you notice any differences in the model's predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e799ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.reset(parameters={__________________}, data=________________)\n",
    "wrapper.run()\n",
    "wrapper.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ddecea",
   "metadata": {},
   "source": [
    "## Exercise 1.3: plot the model output as a function of change in the learning rate\n",
    "\n",
    "Pick a range of learning rates and plot the model output as a function of the change in the learning rate. Try to find learning rates that lead to different behaviours of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c93826",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_not_so_random = numpy.array([__, __, __])\n",
    "\n",
    "big_results = pd.DataFrame()\n",
    "\n",
    "for i in numpy.arange(len(alpha_not_so_random)):\n",
    "    print(f\"Running simulation for participant {i + 1} with alpha={alpha_not_so_random[i]}\")\n",
    "    wrapper.reset(parameters={\"alpha\": alpha_not_so_random[i], \"values\": numpy.ones(4)/4}, data=data[data.ppt == 1])\n",
    "    wrapper.run()\n",
    "    output = wrapper.export()\n",
    "    output[\"alpha\"] = alpha_not_so_random[i]\n",
    "    big_results = pd.concat([big_results, output], ignore_index=True)\n",
    "\n",
    "big_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85efd2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "value_cols = ['Qvalues_0', 'Qvalues_1', 'Qvalues_2', 'Qvalues_3']\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n",
    "\n",
    "for idx, alpha in enumerate(alpha_not_so_random):\n",
    "    ax = axes[idx]\n",
    "    subset = big_results[big_results['alpha'] == alpha]\n",
    "    for vcol, color in zip(value_cols, colors):\n",
    "        ax.plot(subset['trial_0'], subset[vcol], label=vcol, color=color)\n",
    "    ax.set_ylabel('Q-value')\n",
    "    ax.set_title(f'alpha={alpha}')\n",
    "    ax.legend(['stimulus 1', 'stimulus 2', 'stimulus 3', 'stimulus 4'], loc='upper left')\n",
    "axes[-1].set_xlabel('Trial')\n",
    "fig.suptitle('Evolution of Q-values for Different Learning Rates\\n', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9ee96e",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "* What do you notice here? What do you think about the model's behaviour?\n",
    "* How do the parameters affect the model's predictions?\n",
    "* How do the initial Q-values affect the model's predictions?\n",
    "* Anything that surprises you?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f616657f",
   "metadata": {},
   "source": [
    "## (NOT) EXERCISE 1.4: Simulating different participants with the same and different parameters\n",
    "\n",
    "There are built-in tools in the `cpm` toolbox to allow you to explore model behaviour in a variety of ways. The process of trying to understand how the model explains the data often involves exploring its parameter space, simulating different trial orders, and so on. The tool we are using here is called `cpm.generators.Simulator`, which allows you to simulate different participants with the same or different parameters. You can read more about it in the documentation [here](https://devcompsy.github.io/cpm/references/generators/#cpm.generators.Simulator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e3a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = data[data.ppt.isin([1, 3, 9, 4, 10])].copy()\n",
    "numpy.random.seed(42)\n",
    "multiple = parameters.sample(5) ## get 5 random parameter sets for each participant\n",
    "\n",
    "simulate =  cpm.generators.Simulator(\n",
    "    wrapper=wrapper,\n",
    "    parameters=multiple,\n",
    "    data=subset.groupby('ppt'),\n",
    ")\n",
    "simulate.run()\n",
    "simulations_multiple_ppt = simulate.export()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ba44ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "participants = simulations_multiple_ppt['ppt'].unique()\n",
    "n_participants = len(participants)\n",
    "\n",
    "fig, axes = plt.subplots(n_participants, 1, figsize=(12, 3 * n_participants), sharex=True)\n",
    "\n",
    "value_cols = ['values_0', 'values_1', 'values_2', 'values_3']\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n",
    "\n",
    "for idx, ppt in enumerate(participants):\n",
    "    ax = axes[idx] if n_participants > 1 else axes\n",
    "    subset = simulations_multiple_ppt[simulations_multiple_ppt['ppt'] == ppt]\n",
    "    for vcol, color in zip(value_cols, colors):\n",
    "        ax.plot(subset['trial_0'], subset[vcol], label=vcol, color=color)\n",
    "    ax.set_ylabel('Q-value')\n",
    "    ax.set_title(f'Participant {ppt} with alpha={numpy.round(multiple[idx].get(\"alpha\"), 3)}')\n",
    "\n",
    "fig.legend(['stimulus 1', 'stimulus 2', 'stimulus 3', 'stimulus 4'], loc='upper right')\n",
    "axes[-1].set_xlabel('Trial')\n",
    "fig.suptitle('Evolution of Q-values for Different Participants\\n', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6163a2df",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Barto AG, Sutton RS. Reinforcement learning: An introduction. 2nd ed. The MIT Press; 2018.\n",
    "\n",
    "Bridle JS. Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. In: Neurocomputing: Algorithms, architectures and applications. Springer; 1990. p. 227–236.\n",
    "\n",
    "Bush RR, Mosteller F. A mathematical model for simple learning. Psychological review. 1951;58(5):313.\n",
    "\n",
    "Rescorla RA, Wagner AR. A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement. In: Black AH, Prokasy WF, editors. Classical Conditioning II: Current Research and Theory. Appleton-Century-Crofts; 1972. p. 64–99.\n",
    "\n",
    "Rumelhart DE, Hinton GE, Williams RJ. Learning representations by back-propagating errors. nature. 1986;323(6088):533–536.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
