{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b1d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cpm-toolbox\n",
    "import cpm\n",
    "from packaging import version\n",
    "\n",
    "## cpm checks\n",
    "print(cpm.__version__)\n",
    "if version.parse(cpm.__version__) < version.parse(\"0.22\"):\n",
    "    raise ImportError(\"cpm version must be >= 0.22. Please install the latest version using: pip install --upgrade cpm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93549d22",
   "metadata": {},
   "source": [
    "# EXERCISE 2\n",
    "\n",
    "In this exercise, you will use the model you implemented in Exercise 1 to fit some real-world data. Then you will repeat the same process with a hierarchical modeling approach.\n",
    "\n",
    "## EXERCISE 2.1: Import the data and your model from Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e7583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import cpm\n",
    "import cpm.datasets as datasets\n",
    "from cpm.generators import Parameters, Value\n",
    "import ipyparallel as ipp  # for parallel computing with ipython (specific for Jupyter Notebook)\n",
    "\n",
    "data = datasets.load_bandit_data()\n",
    "data.head()\n",
    "\n",
    "parameters = Parameters(\n",
    "    # free parameters are indicated by specifying priors\n",
    "    alpha=Value(\n",
    "        value=0.5,\n",
    "        lower=1e-10,\n",
    "        upper=1,\n",
    "        prior=\"truncated_normal\",\n",
    "        args={\"mean\": 0.25, \"sd\": 0.25},\n",
    "    ),\n",
    "    temperature=Value(\n",
    "        value=1,\n",
    "        lower=0,\n",
    "        upper=10,\n",
    "        prior=\"truncated_normal\",\n",
    "        args={\"mean\": 5, \"sd\": 2.5},\n",
    "    ),\n",
    "    # everything without a prior is part of the initial state of the\n",
    "    # model or constructs fixed throughout the simulation\n",
    "    # (e.g. exemplars in general-context models of categorizations)\n",
    "    # initial q-values starting starting from non-zero value\n",
    "    # these are equal to all 4 stimuli (1 / 4)\n",
    "    values = numpy.array([0.25, 0.25, 0.25, 0.25])\n",
    "    )\n",
    "\n",
    "@ipp.require(\"numpy\")\n",
    "def model(parameters, trial):\n",
    "    # pull out the parameters\n",
    "    alpha = parameters.alpha\n",
    "    temperature = parameters.temperature\n",
    "    values = numpy.array(parameters.values)\n",
    "    \n",
    "    # pull out the trial information\n",
    "    stimulus = numpy.array([trial.arm_left, trial.arm_right]).astype(int)\n",
    "    feedback = numpy.array([trial.reward_left, trial.reward_right])\n",
    "    human_choice = trial.response.astype(int)\n",
    "\n",
    "    # Equation 1. - get the value of each available action\n",
    "    # Note that because python counts from 0, we need to shift\n",
    "    # the stimulus identifiers by -1\n",
    "    expected_rewards = values[stimulus - 1]\n",
    "    # convert columns to rows\n",
    "    expected_rewards = expected_rewards.reshape(2, 1)\n",
    "    # calculate a policy based on the activations\n",
    "    # Equation 2.\n",
    "    choice_rule = cpm.models.decision.Softmax(\n",
    "        activations=expected_rewards,\n",
    "        temperature=temperature\n",
    "        )\n",
    "    choice_rule.compute() # compute the policy\n",
    "    # if the policy is NaN for an action, then we need to set it to 1\n",
    "    # this corrects some numerical issues with python and infinities\n",
    "    if numpy.isnan(choice_rule.policies).any():\n",
    "        choice_rule.policies[numpy.isnan(choice_rule.policies)] = 1\n",
    "    # get the received reward for the choice\n",
    "    reward = feedback[human_choice]\n",
    "    teacher = numpy.array([reward])\n",
    "    # we now create a vector that tells our learning rule what...\n",
    "    # ... stimulus to update according to the participant's choice\n",
    "    what_to_update = numpy.zeros(4)\n",
    "    chosen_stimulus = stimulus[human_choice] - 1\n",
    "    what_to_update[chosen_stimulus] = 1\n",
    "\n",
    "    # Equation 4.\n",
    "    update = cpm.models.learning.SeparableRule(\n",
    "                    weights=values,\n",
    "                    feedback=teacher,\n",
    "                    input=what_to_update,\n",
    "                    alpha=alpha\n",
    "                    )\n",
    "    update.compute()\n",
    "    # Equation 5.\n",
    "    values += update.weights.flatten()\n",
    "    # compile output\n",
    "    output = {\n",
    "        \"trial\"    : trial.trial.astype(int), # trial numbers\n",
    "        \"activation\" : expected_rewards.flatten(), # expected reward of arms\n",
    "        \"policy\"   : choice_rule.policies,       # policies\n",
    "        \"reward\"   : reward,                  # received reward\n",
    "        \"error\"    : update.weights,          # prediction error\n",
    "        \"values\"   : values,                  # updated values\n",
    "        # dependent variable\n",
    "        \"dependent\"  : numpy.array([choice_rule.policies[1]]),\n",
    "    }\n",
    "    return output\n",
    "\n",
    "generative_model = cpm.generators.Wrapper(\n",
    "    model=model,\n",
    "    parameters=parameters,\n",
    "    data=data[data == 1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e7ede9",
   "metadata": {},
   "source": [
    "## EXERCISE 2.2: Fit the model to the data\n",
    "\n",
    "\n",
    "### EXERCISE 2.2.1: Specify your discrepency function\n",
    "\n",
    "\n",
    "Here is a reminder of the loss function from the slides:\n",
    "\n",
    "\\begin{align*}\n",
    "-\\log L(\\theta \\mid Y, M) = -\\sum_{i=1}^{N} \\log \\bigg[ p(y_i \\mid \\theta) \\bigg]\n",
    "\\end{align*}\n",
    "\n",
    "where $\\theta$ are model parameters, $Y$ is the data with $N$ number of data points, and $M$ is the model. The $p(y~|~\\theta)$ gives the probability of observing the data given a certain parameter set. It follows a _Bernoulli_ distribution and defined such as:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "p(y_t~|~\\theta) = \\left\\{ \\begin{array}{ll} P(a_t) &\n",
    "\\text{if } y = 1, \\text{ and} \\\\ 1 - P(a_t) & \\text{if } y = 0.\n",
    "\\end{array} \\right.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Rest assured, you do not have to worry about the math here, as the `cpm` toolbox will take care of it for you. `cpm.optimisation.minimise` has various options for the discrepancy function, including `cpm.optimisation.minimise.LogLikelihood.bernoulli`. See its documentation [here](https://devcompsy.github.io/cpm/api/optimisation/#cpm.optimisation.minimise.LogLikelihood.bernoulli). Under the hood, it will calculate the log-likelihood of the Bernoulli distribution for you, given the data and the model parameters, and return the negative log-likelihood as the discrepancy function. This is an example code for the discrepancy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c93b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss_function(observed, predicted):\n",
    "        limit = np.log(1e-200)\n",
    "        bound = np.finfo(np.float64).min\n",
    "        probabilities = predicted.flatten()\n",
    "        numpy.clip(probabilities, 1e-100, 1 - 1e-100, out=probabilities)\n",
    "\n",
    "        LL = bernoulli.logpmf(k=observed.flatten(), p=probabilities)\n",
    "        LL[LL < bound] = limit  # Set the lower bound to avoid overflow\n",
    "        NLL = -1 * np.sum(LL)\n",
    "        return NLL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f6007",
   "metadata": {},
   "source": [
    "### EXERCISE 2.2.3: Fit the model\n",
    "\n",
    "First, you will specify the `observed` variable in the data, then use `FminBound` to fit the model to the data. You will need to specify the **model you are running, the data, the minimisation (discrepency) function**. See the documentaiton [here](https://devcompsy.github.io/cpm/api/optimisation/#cpm.optimisation.FminBound) for details on what you can specify and how to fill in the blanks below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1331183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cpm.optimisation import minimise, FminBound\n",
    "\n",
    "data[\"observed\"] = data[\"response\"].astype(int)  # convert response to int\n",
    "\n",
    "# Set up the fitting procedure\n",
    "fit = FminBound(\n",
    "    model=__________________,  # Wrapper class with the model we specified from before\n",
    "    data=___________________,  # the data grouped by participant column\n",
    "    minimisation=___________,  # use the log-likelihood for Bernoulli distribution\n",
    "    parallel=True,\n",
    "    libraries=[\"numpy\", \"cpm\", \"pandas\"],\n",
    "    prior=False,\n",
    "    ppt_identifier=\"ppt\",\n",
    "    display=False,\n",
    "    number_of_starts=5,\n",
    "    # everything below is optional and passed directly to the scipy implementation of the optimiser\n",
    "    approx_grad=True\n",
    "\n",
    ")\n",
    "\n",
    "# Run the fitting procedure\n",
    "_________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34571fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_estimates_non_hierarchical = fit.export()\n",
    "parameter_estimates_non_hierarchical.rename(\n",
    "    columns={\n",
    "        \"x_0\": \"alpha\",\n",
    "        \"x_1\": \"temperature\",\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "parameter_estimates_non_hierarchical.alpha.hist(bins=20, ax=axs[0])\n",
    "parameter_estimates_non_hierarchical.temperature.hist(bins=20, ax=axs[1])\n",
    "\n",
    "axs[0].set_title(\"Distribution of alpha (learning rate)\")\n",
    "axs[1].set_title(\"Distribution of temperature (inverse temperature)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fc7247",
   "metadata": {},
   "source": [
    "## EXERCISE 2.3: Hierarchical modeling\n",
    "\n",
    "Now, how do we turn our model into a hierarchical model? The idea is to assume that the parameters of the model are not fixed, but rather drawn from a distribution. The variance between subject-level parameters are constrained through the priors drawn over the parameter space. But, hey! We have already defined the priors in the model parameters, so we can just use those. The way to use them is to flip a switch in the optimisation function by setting the option related to priors True. Find the relevant function argument in the online [documentation of FminBound](https://devcompsy.github.io/cpm/api/optimisation/#cpm.optimisation.FminBound) and insert it into the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649774fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cpm.optimisation import minimise, FminBound\n",
    "\n",
    "data[\"observed\"] = data[\"response\"].astype(int)  # convert response to int\n",
    "\n",
    "# Set up the fitting procedure\n",
    "fit = FminBound(\n",
    "    model=generative_model,  # Wrapper class with the model we specified from before\n",
    "    data=data.groupby('ppt'),  # the data as a list of dictionaries\n",
    "    minimisation=minimise.LogLikelihood.bernoulli,\n",
    "    parallel=True,\n",
    "    libraries=[\"numpy\", \"cpm\", \"pandas\"],\n",
    "    ppt_identifier=\"ppt\",\n",
    "    display=False,\n",
    "    number_of_starts=5,\n",
    "    cl=5,\n",
    "    ___________________, # !!! use priors for the parameters\n",
    "    # everything below is optional and passed directly to the scipy implementation of the optimiser\n",
    "    approx_grad=True\n",
    "\n",
    ")\n",
    "\n",
    "## you have to run the optimisation, look up the appropriate method in the documentation\n",
    "_____________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a515c27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_estimates_with_hierarchical = fit.export()\n",
    "parameter_estimates_with_hierarchical.rename(\n",
    "    columns={\n",
    "        \"x_0\": \"alpha\",\n",
    "        \"x_1\": \"temperature\",\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "parameter_estimates_non_hierarchical.alpha.hist(bins=20, ax=axs[0])\n",
    "parameter_estimates_non_hierarchical.temperature.hist(bins=20, ax=axs[1])\n",
    "\n",
    "axs[0].set_title(\"Distribution of alpha (learning rate) with priors\")\n",
    "axs[1].set_title(\"Distribution of temperature (inverse temperature) with priors\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9059f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax[0].plot(\n",
    "    parameter_estimates_with_hierarchical.alpha,\n",
    "    parameter_estimates_non_hierarchical.alpha,\n",
    "    \"o\",\n",
    "    alpha=0.5,\n",
    "    label=\"Non-hierarchical\"\n",
    ")\n",
    "ax[0].set_ylabel(r\"$\\alpha$ without priors\")\n",
    "ax[0].set_xlabel(r\"$\\alpha$ with priors\")\n",
    "ax[0].set_title(r\"$\\alpha$ (learning rate) parameter comparison\")\n",
    "ax[0].plot([0, 1], [0, 1], \"k--\", alpha=0.3)  # reference line\n",
    "\n",
    "ax[1].plot(\n",
    "    parameter_estimates_with_hierarchical.temperature,\n",
    "    parameter_estimates_non_hierarchical.temperature,\n",
    "    \"o\",\n",
    "    alpha=0.5,\n",
    "    label=\"Non-hierarchical\"\n",
    ")\n",
    "ax[1].set_ylabel(r\"$\\beta$ without priors\")\n",
    "ax[1].set_xlabel(r\"$\\beta$ with priors\")\n",
    "ax[1].set_title(r\"$\\beta$ (inverse temperature) parameter comparison\")\n",
    "ax[1].plot([0, 10], [0, 10], \"k--\", alpha=0.3)  # reference line\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb4c09e",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "- What is the difference between the model you fitted first and second?\n",
    "- What did you learn about the impact of priors on parameter estimation?\n",
    "- What differences you notice between the results of the two parameter estimations? Is it better, worse, or the something else?\n",
    "- In what situations would you prefer a hierarchical model over a non-hierarchical one?\n",
    "- How confident are you in the parameter estimates, and what could increase your confidence?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3e65c8",
   "metadata": {},
   "source": [
    "## Extra information\n",
    "\n",
    "The `cpm` toolbox is designed to be modular. Which means that the tools we implement work together, such that once you have a model, you can use it with any of the optimisation methods we provide. This is why we can use the same model with `FminBound` and `DifferentialEvolution`, or why you can flip a switch to enable hierarchical modeling.\n",
    "The feature I want to draw your attention is that once we estimated model parameters, we can use them to simulate latent variables for each participant. Simple take the `fit.parameters` and use them in `cpm.generators.Simulator` to simulate with the model on participant's data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7683a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate =  cpm.generators.Simulator(\n",
    "    wrapper=generative_model,\n",
    "    parameters=fit.parameters,\n",
    "    data=data.groupby('ppt'),\n",
    ")\n",
    "simulate.run()\n",
    "simulations_multiple_ppt = simulate.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735abf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "participants = simulations_multiple_ppt['ppt'].unique()\n",
    "n_participants = len(participants)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "value_cols = ['values_0', 'values_1', 'values_2', 'values_3']\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n",
    "\n",
    "\n",
    "for vcol, color in zip(value_cols, colors):\n",
    "    # Plot mean Q-value across participants for each stimulus\n",
    "    mean_vals = simulations_multiple_ppt.groupby('trial_0')[vcol].mean()\n",
    "    ax.plot(mean_vals.index, mean_vals.values, color=color, linewidth=2, label=f'{vcol} mean')\n",
    "\n",
    "ax.set_ylabel('Q-value')\n",
    "ax.set_xlabel('Trial')\n",
    "ax.set_title('Evolution of Q-values (all participants, mean bold)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
