{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a926cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cpm\n",
    "from packaging import version\n",
    "\n",
    "## cpm checks\n",
    "print(cpm.__version__)\n",
    "if version.parse(cpm.__version__) < version.parse(\"0.22\"):\n",
    "    raise ImportError(\"cpm version must be >= 0.22. Please install the latest version using: pip install --upgrade cpm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7928c735",
   "metadata": {},
   "source": [
    "# EXERCISE 2\n",
    "\n",
    "Evaluating how well the model recovers the original parameters. Briefly, you will sample a random set of model parameters, generate new data, fit the model to the data, and then compare the estimated parameters to the original ones. This will require some modification to the code you wrote in Exercise 1. Specifically, instead of using the participant's choice to select what value to update, you will take the model's probability of choosing each option and use that to sample a choice. The technique we will use below will also involve two new additions to our model: the _loss function_ and the _Bernoulli distribution function_ to generate data. So, what will you do here?\n",
    "\n",
    "2. Estimate the parameter recovery of the model\n",
    "    1. Generate data from the model\n",
    "    2. Fit the model to the data\n",
    "    3. Compare the estimated parameters to the true parameters\n",
    "\n",
    "\n",
    "## Discrepency function (Negative Log Likelihood)\n",
    "\n",
    "So, because we will now do some parameter estimation, we need to include some quantitative measure about how well the model is doing at fitting our data. Here, we define the loss function based on negative log likelihood.\n",
    "\n",
    "\\begin{align*}\n",
    "-\\log L(\\theta \\mid Y, M) = -\\sum_{i=1}^{N} \\log \\bigg[ p(y_i \\mid \\theta) \\bigg]\n",
    "\\end{align*}\n",
    "\n",
    "where $\\theta$ are model parameters, $Y$ is the data with $N$ number of data points, and $M$ is the model. The $p(y~|~\\theta)$ gives the probability of observing the data given a certain parameter set. It follows a _Bernoulli_ distribution and defined such as:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "p(y_t~|~\\theta) = \\left\\{ \\begin{array}{ll} P(a_t) &\n",
    "\\text{if } y = 1, \\text{ and} \\\\ 1 - P(a_t) & \\text{if } y = 0.\n",
    "\\end{array} \\right.\n",
    "\\end{align*}\n",
    "\n",
    "Note that the loss function is not part of our model specification, but is implemented as `cpm.optimistaion.minimisation.LogLikelihood.bernoulli`. If the number of actions participants can take on a given trial is greater than one, we can exchange it for a loss function `LogLikelihood.categorical` without the need to modify the implementation of a model.\n",
    "\n",
    "## Generating data with the model\n",
    "\n",
    "To generate data with the model, we will use the same _Bernoulli distribution_, with its mean given by the model's probability of choosing each option. The SoftMax function we defined previously will return a probability for each action, and we will sample from this distribution to generate a choice. This is done using the `cpm.models.decision.SoftMax` class. You will need to look up the documentation and figure out what method you need to use to sample from the choice probabilities. You can also decide to do it on your own and write your own function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583d2015",
   "metadata": {},
   "source": [
    "## EXERCISE 2.1: modify your model, generate data with random parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2183ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import cpm\n",
    "import cpm.datasets as datasets\n",
    "from cpm.generators import Parameters, Value\n",
    "import ipyparallel as ipp  # for parallel computing with ipython (specific for Jupyter Notebook)\n",
    "\n",
    "data = datasets.load_bandit_data()\n",
    "data.head()\n",
    "\n",
    "parameters = Parameters(\n",
    "    # free parameters are indicated by specifying priors\n",
    "    alpha=Value(\n",
    "        value=0.5,\n",
    "        lower=1e-10,\n",
    "        upper=1,\n",
    "        prior=\"truncated_normal\",\n",
    "        args={\"mean\": 0.5, \"sd\": 0.25},\n",
    "    ),\n",
    "    temperature=Value(\n",
    "        value=1,\n",
    "        lower=0,\n",
    "        upper=10,\n",
    "        prior=\"truncated_normal\",\n",
    "        args={\"mean\": 5, \"sd\": 2.5},\n",
    "    ),\n",
    "    # everything without a prior is part of the initial state of the\n",
    "    # model or constructs fixed throughout the simulation\n",
    "    # (e.g. exemplars in general-context models of categorizations)\n",
    "    # initial q-values starting starting from non-zero value\n",
    "    # these are equal to all 4 stimuli (1 / 4)\n",
    "    values = numpy.array([0.25, 0.25, 0.25, 0.25])\n",
    "    )\n",
    "\n",
    "@ipp.require(\"numpy\")\n",
    "def model(parameters, trial):\n",
    "    # pull out the parameters\n",
    "    alpha = parameters.alpha\n",
    "    temperature = parameters.temperature\n",
    "    values = numpy.array(parameters.values)\n",
    "    \n",
    "    # pull out the trial information\n",
    "    stimulus = numpy.array([trial.arm_left, trial.arm_right]).astype(int)\n",
    "    feedback = numpy.array([trial.reward_left, trial.reward_right])\n",
    "\n",
    "    # Equation 1. - get the value of each available action\n",
    "    # Note that because python counts from 0, we need to shift\n",
    "    # the stimulus identifiers by -1\n",
    "    expected_rewards = values[stimulus - 1]\n",
    "    # convert columns to rows\n",
    "    expected_rewards = expected_rewards.reshape(2, 1)\n",
    "    # calculate a policy based on the activations\n",
    "    # Equation 2.\n",
    "    choice_rule = cpm.models.decision.Softmax(\n",
    "        activations=expected_rewards,\n",
    "        temperature=temperature\n",
    "        )\n",
    "    choice_rule.compute() # compute the policy\n",
    "    # if the policy is NaN for an action, then we need to set it to 1\n",
    "    # this corrects some numerical issues with python and infinities\n",
    "    if numpy.isnan(choice_rule.policies).any():\n",
    "        choice_rule.policies[numpy.isnan(choice_rule.policies)] = 1\n",
    "    # get the model's choice\n",
    "    model_choices = _________________________________\n",
    "    # get the received reward for the choice\n",
    "    reward = feedback[model_choices]\n",
    "    teacher = numpy.array([reward])\n",
    "    # we now create a vector that tells our learning rule what...\n",
    "    # ... stimulus to update according to the participant's choice\n",
    "    what_to_update = numpy.zeros(4)\n",
    "    chosen_stimulus = stimulus[model_choices] - 1\n",
    "    what_to_update[chosen_stimulus] = 1\n",
    "\n",
    "    # Equation 4.\n",
    "    update = cpm.models.learning.SeparableRule(\n",
    "                    weights=values,\n",
    "                    feedback=teacher,\n",
    "                    input=what_to_update,\n",
    "                    alpha=alpha\n",
    "                    )\n",
    "    update.compute()\n",
    "    # Equation 5.\n",
    "    values += update.weights.flatten()\n",
    "    # compile output\n",
    "    output = {\n",
    "        \"trial\"    : trial.trial.astype(int), # trial numbers\n",
    "        \"activation\" : expected_rewards.flatten(), # expected reward of arms\n",
    "        \"policy\"   : choice_rule.policies,       # policies\n",
    "        \"reward\"   : reward,                  # received reward\n",
    "        \"error\"    : update.weights,          # prediction error\n",
    "        \"values\"   : values,                  # updated values\n",
    "        \"response\" : _______________          # model's choice\n",
    "        # dependent variable\n",
    "        \"dependent\"  : numpy.array([choice_rule.policies[1]]),\n",
    "    }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08399d15",
   "metadata": {},
   "source": [
    "# EXERCISE 2.2: generate data with random parameters\n",
    "\n",
    "Her, you need to generate a random set of parameters, equal in number to the number of participants in your data set. Then, you will use the `cpm.generators.Simulator` class to generate data for the whole sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb19520",
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model = cpm.generators.Wrapper(\n",
    "    ______________________________,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee49754",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_participants = ____________________________________________________\n",
    "parameters_for_the_whole_sample = parameters.sample(____________________________________________________)\n",
    "original_parameters = pd.DataFrame(parameters_for_the_whole_sample) ## save the original parameters\n",
    "original_parameters[\"ppt\"] = data.ppt.unique()\n",
    "\n",
    "# create a simulator that will run the model for each participant\n",
    "simulator = cpm.generators.Simulator(\n",
    "    ____________________________________,\n",
    ")\n",
    "## run the simulator\n",
    "____________________________________________________\n",
    "# collect the results\n",
    "results = ___________________________,\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54993839",
   "metadata": {},
   "source": [
    "All that is left from the simulations is to take the model output and create our dependent variable for the fitting. The toolbox requires the data to contain a single column with the `dependent variable`, that we are estimating model performance against. In this case, the dependent variable is the choice made by the model, which is a binary variable (0 or 1) indicating whether the model chose option 1 or option 2. We take that from the `Simulator` output and create a new column in the data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff5e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_data = data.copy() ## copy the original data\n",
    "recovery_data[\"observed\"] = ____________________________________________________\n",
    "recovery_data[\"response\"] = ____________________________________________________\n",
    "\n",
    "recovery_data.head() ## print the first few rows of the data to check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bcae4d",
   "metadata": {},
   "source": [
    "## EXERCISE 2.2: fit the model to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1668b4",
   "metadata": {},
   "source": [
    "## EXERCISE 2.2.1: revert the model to the original version\n",
    "\n",
    "So, we need to revert the model to the original version, which means removing the `choice` argument from the code we wrote before. This will allow us to fit the model to the data generated in Exercise 2.2. as opposed to generating new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60804b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ipp.require(\"numpy\")\n",
    "def model_fitting(parameters, trial):\n",
    "    # pull out the parameters\n",
    "    alpha = parameters.alpha\n",
    "    temperature = parameters.temperature\n",
    "    values = numpy.array(parameters.values)\n",
    "    \n",
    "    # pull out the trial information\n",
    "    stimulus = numpy.array([trial.arm_left, trial.arm_right]).astype(int)\n",
    "    feedback = numpy.array([trial.reward_left, trial.reward_right])\n",
    "    human_choice = trial.observed.astype(int)\n",
    "\n",
    "    # Equation 1. - get the value of each available action\n",
    "    # Note that because python counts from 0, we need to shift\n",
    "    # the stimulus identifiers by -1\n",
    "    expected_rewards = values[stimulus - 1]\n",
    "    # convert columns to rows\n",
    "    expected_rewards = expected_rewards.reshape(2, 1)\n",
    "    # calculate a policy based on the activations\n",
    "    # Equation 2.\n",
    "    choice_rule = cpm.models.decision.Softmax(\n",
    "        activations=expected_rewards,\n",
    "        temperature=temperature\n",
    "        )\n",
    "    choice_rule.compute() # compute the policy\n",
    "    # if the policy is NaN for an action, then we need to set it to 1\n",
    "    # this corrects some numerical issues with python and infinities\n",
    "    if numpy.isnan(choice_rule.policies).any():\n",
    "        choice_rule.policies[numpy.isnan(choice_rule.policies)] = 1\n",
    "    # get the received reward for the choice\n",
    "    reward = feedback[human_choice]\n",
    "    teacher = numpy.array([reward])\n",
    "    # we now create a vector that tells our learning rule what...\n",
    "    # ... stimulus to update according to the participant's choice\n",
    "    what_to_update = numpy.zeros(4)\n",
    "    chosen_stimulus = stimulus[human_choice] - 1\n",
    "    what_to_update[chosen_stimulus] = 1\n",
    "\n",
    "    # Equation 4.\n",
    "    update = cpm.models.learning.SeparableRule(\n",
    "                    weights=values,\n",
    "                    feedback=teacher,\n",
    "                    input=what_to_update,\n",
    "                    alpha=alpha\n",
    "                    )\n",
    "    update.compute()\n",
    "    # Equation 5.\n",
    "    values += update.weights.flatten()\n",
    "    # compile output\n",
    "    output = {\n",
    "        \"trial\"    : trial.trial.astype(int), # trial numbers\n",
    "        \"activation\" : expected_rewards.flatten(), # expected reward of arms\n",
    "        \"policy\"   : choice_rule.policies,       # policies\n",
    "        \"reward\"   : reward,                  # received reward\n",
    "        \"error\"    : update.weights,          # prediction error\n",
    "        \"values\"   : values,                  # updated values\n",
    "        # dependent variable\n",
    "        \"dependent\"  : numpy.array([choice_rule.policies[1]]),\n",
    "    }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef4b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = cpm.generators.Wrapper(\n",
    "    ____________________________________,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1778145",
   "metadata": {},
   "source": [
    "Now that we have everything ready, we can fit the model to the data. We will use the `cpm.optimisation.FminBound` class to do that. The fitting process will return a set of estimated parameters for each participant, which we can then compare to the original parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edd6fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cpm.optimisation import minimise, FminBound\n",
    "# Set up the fitting procedure\n",
    "fit = FminBound(\n",
    "    _____________________________________,\n",
    "    # everything below is optional and passed directly to the scipy implementation of the optimiser\n",
    "    approx_grad=True ## we do not have a ready-to-use gradient for this model, so we use the approximate gradient\n",
    "\n",
    ")\n",
    "\n",
    "fit.optimise()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8fc088",
   "metadata": {},
   "source": [
    "## EXERCISE 2.3: compare the estimated parameters with the original ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5561acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_parameters = fit.export()\n",
    "recovered_parameters.rename(\n",
    "    columns={\n",
    "        \"x_0\": \"alpha\",\n",
    "        \"x_1\": \"temperature\",\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "recovered_parameters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391d91ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "alpha_original = ___________________________________\n",
    "alpha_recovered = ___________________________________\n",
    "beta_original = ____________________________________\n",
    "beta_recovered = _____________________________________\n",
    "# Plotting the results\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=False)\n",
    "\n",
    "## scatterplots\n",
    "axes[0].scatter(alpha_original, alpha_recovered, alpha=0.5)\n",
    "## add correlation coefficient\n",
    "corr_alpha = numpy.corrcoef(alpha_original, alpha_recovered)[0, 1]\n",
    "axes[0].text(0.5, 0.9, f'Correlation: {corr_alpha:.2f}', transform=axes[0].transAxes, fontsize=12, ha='center', va='center')\n",
    "axes[0].set_xlabel(r'Original $\\alpha$')\n",
    "axes[0].set_ylabel(r'Recovered $\\alpha$')\n",
    "axes[0].set_title('Learning rate recovery\\n')\n",
    "axes[0].plot([0, 1], [0, 1], 'r--')  # Diagonal line for reference\n",
    "axes[1].scatter(beta_original, beta_recovered, alpha=0.5)\n",
    "## add correlation coefficient\n",
    "corr_beta = numpy.corrcoef(beta_original, beta_recovered)[0, 1]\n",
    "axes[1].text(0.5, 0.9, f'Correlation: {corr_beta:.2f}', transform=axes[1].transAxes, fontsize=12, ha='center', va='center')\n",
    "axes[1].set_xlabel(r'Original $\\beta$')\n",
    "axes[1].set_ylabel(r'Recovered $\\beta$')\n",
    "axes[1].set_title('Choice stochasticity (inverse temperature) recovery\\n')\n",
    "axes[1].plot([0, 10], [0, 10], 'r--')  # Diagonal line for reference\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959e83c4",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "- What do you think about the results of the parameter recovery? Are the parameters identifiable?\n",
    "- Are there any parameters that are not identifiable? If so, why do you think that is the case?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
